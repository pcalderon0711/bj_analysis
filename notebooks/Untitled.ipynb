{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from neurotools.io.files import load_embedding_model\n",
    "from neurotools.io.gdrive import download_sheets_doc\n",
    "from neurotools.plot import figure\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from neurotools.language.simple_tokenizer import SimpleTokenizer\n",
    "from wordcloud import WordCloud\n",
    "from googletrans import Translator\n",
    "import itertools\n",
    "import ast\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra characters read for de\n",
      "Stopwords set read for de\n",
      "Lemma lookup read for de\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizer('de') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb, word2rank_dict = load_embedding_model('../utils/002_de_de_commoncrawl_v1_20171209.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_vector(words, emb):\n",
    "    \"\"\"\n",
    "        Average the word vectors of a list of words.\n",
    "    \"\"\"\n",
    "    words_in_emb = [emb[word] for word in words if word in emb.vocab.keys()]\n",
    "    return np.mean(words_in_emb, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf_map(documents):\n",
    "    tfidf = TfidfVectorizer(use_idf=True)\n",
    "    tfidf.fit_transform(documents)\n",
    "    word_to_idf = dict((x[0], tfidf.idf_[x[1]]) for x in sorted(tfidf.vocabulary_.items(), key=operator.itemgetter(1)))\n",
    "    return word_to_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_idf_weighted_average_vector(words, word_to_idf, emb):\n",
    "    \"\"\"\n",
    "        Average the idf-weighted word vectors of a list of words. (Map this function to every document)\n",
    "    \"\"\"\n",
    "    # only consider words IN embedding model and in TFIDF vectorizer\n",
    "    words = [word for word in words if word in word_to_idf.keys() and word in emb.vocab.keys()]\n",
    "    weighted = [word_to_idf[word] * emb[word] for word in words]\n",
    "    return np.mean(weighted, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    \"\"\"\n",
    "        Compute the cosine similarity of a and b.\n",
    "    \"\"\"\n",
    "    return np.dot(a,b) / ( (np.dot(a,a) **.5) * (np.dot(b,b) ** .5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similarity_with_dimensions(row, dimensions):\n",
    "    \"\"\"\n",
    "        Get similarity of row's word vector and every dimension.\n",
    "        row : row of the movie dataframe\n",
    "        dimensions : dataframe containing Neuroflash dimensions\n",
    "    \"\"\"\n",
    "    for k in range(dimensions['dimension'].shape[0]):\n",
    "        name = str(dimensions['label'].iloc[k])\n",
    "        # define a new column for the cosine similarity of dimension k and the post text\n",
    "        row[name] = cosine_similarity(row['wv'], dimensions['dimension'].iloc[k])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions = pd.read_csv('../data/all_dimensions.csv')\n",
    "dimensions.dimension = dimensions.dimension.map(lambda x: np.array(ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# because some display as 1.4K\n",
    "def convert_K_to_num(x):\n",
    "    if 'K' in str(x):\n",
    "        num = float(str(x[:-1])) * 1000\n",
    "    elif pd.isnull(x):\n",
    "        num = 0\n",
    "    else:\n",
    "        num = float(x)\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "insta1 = pd.read_csv('../data/Insta_Ben_Jerrys/Instagram Data Link2 part 1.csv')\n",
    "insta2 = pd.read_csv('../data/Insta_Ben_Jerrys/Instagram Data Link2 part 2.csv')\n",
    "insta3 = pd.read_csv('../data/Insta_Ben_Jerrys/Instagram Data Link2 part 3.csv')\n",
    "insta4 = pd.read_csv('../data/Insta_Ben_Jerrys/Instagram Data Link2 part 4.csv')\n",
    "benandjerrys = pd.concat([insta1, insta2, insta3, insta4], axis=0, ignore_index=True)\n",
    "\n",
    "insta1 = pd.read_csv('../data/Insta_MissesVlog//Instagram Data Link3 part 1.csv')\n",
    "insta2 = pd.read_csv('../data/Insta_MissesVlog/Instagram Data Link3 part 2.csv')\n",
    "insta3 = pd.read_csv('../data/Insta_MissesVlog/Instagram Data Link3 part 3.csv')\n",
    "insta4 = pd.read_csv('../data/Insta_MissesVlog/Instagram Data Link3 part 4.csv')\n",
    "insta5 = pd.read_csv('../data/Insta_MissesVlog/Instagram Data Link3 part 5.csv')\n",
    "insta6 = pd.read_csv('../data/Insta_MissesVlog/Instagram Data Link3 part 6.csv')\n",
    "insta7 = pd.read_csv('../data/Insta_MissesVlog/Instagram Data Link3 part 7.csv')\n",
    "missesvlog = pd.concat([insta1, insta2, insta3, insta4, insta5, insta6, insta7], axis=0, ignore_index=True)\n",
    "\n",
    "insta1 = pd.read_csv('../data/Insta_Aminatabelli/Instagram Data Link3 part one.csv')\n",
    "insta2 = pd.read_csv('../data/Insta_Aminatabelli/Instagram Data Link3 part two.csv')\n",
    "insta3 = pd.read_csv('../data/Insta_Aminatabelli/Instagram Data Link3 part three.csv')\n",
    "aminatabelli = pd.concat([insta1, insta2, insta3], axis=0, ignore_index=True)\n",
    "\n",
    "insta1 = pd.read_csv('../data/Insta_Jung_Naiv/Instagram Data Link 1 part one.csv')\n",
    "insta2 = pd.read_csv('../data/Insta_Jung_Naiv/Instagram Data Link 1 part two.csv')\n",
    "jung_naiv = pd.concat([insta1, insta2], axis=0, ignore_index=True)\n",
    "\n",
    "greenpeace = pd.read_csv('../data/Insta_Greenpeace.csv')\n",
    "lemonaid = pd.read_csv('../data/Insta_Lemonaid.csv')\n",
    "victorisvanviolence = pd.read_csv('../data/Insta_VictorisVanViolence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs = [benandjerrys, missesvlog, aminatabelli, jung_naiv, greenpeace, lemonaid, victorisvanviolence]\n",
    "names = ['IG_BenandJerrys.csv', 'IG_MissesVlog.csv', 'IG_AminataBelli.csv', 'IG_jungundnaiv.csv', 'IG_Greenpeace.csv', 'IG_Lemonaid.csv', 'IG_Victoria.van.Violence.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterate = list(zip(dfs, names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_insta_data(insta, name):\n",
    "    num_of_comments = insta.groupby('Post ID')['Post Comment'].count() - 1\n",
    "    num_of_comments.name = 'comments'\n",
    "    insta_df = insta.groupby('Post ID').head(1).set_index('Post ID')\n",
    "    insta_df['likes'] = insta_df['Post likes'].str.replace(',', '').str.extract('([0-9]+)').astype(int)\n",
    "    insta_df = insta_df.join(num_of_comments, how='left')\n",
    "    insta_df = insta_df.reset_index()\n",
    "    insta_df['success'] = insta_df['likes'] + 2*insta_df['comments']\n",
    "    insta_df['tokens'] = insta_df['Post Comment'].astype(str).map(tokenizer.tokenize)\n",
    "    insta_df = insta_df[['Post Comment','tokens', 'success', 'likes', 'comments', 'Post Time']]\n",
    "    insta_df.columns = ['message', 'tokens', 'success', 'likes', 'comments', 'created_time']\n",
    "    insta_word_to_idf = get_idf_map(insta_df['tokens'].map(lambda x: ' '.join(x)).values)\n",
    "    insta_df['wv'] = insta_df['tokens'].map(lambda x: get_idf_weighted_average_vector(x, insta_word_to_idf, emb))\n",
    "    insta_df = insta_df[~insta_df['wv'].isnull()]\n",
    "    insta_df = insta_df.apply(lambda x: get_similarity_with_dimensions(x, dimensions), axis=1)\n",
    "    insta_df['wv'] = insta_df['wv'].map(lambda x: str(list(x)))\n",
    "    insta_df.to_csv('../processed/processed_' + name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing IG_jungundnaiv.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piocalderon/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  \"\"\"\n",
      "/Users/piocalderon/anaconda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/piocalderon/anaconda/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing IG_Greenpeace.csv\n",
      "Doing IG_Lemonaid.csv\n",
      "Doing IG_Victoria.van.Violence.csv\n"
     ]
    }
   ],
   "source": [
    "for df, name in iterate[3:]:\n",
    "    print(\"Doing {}\".format(name))\n",
    "    preprocess_insta_data(df, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
